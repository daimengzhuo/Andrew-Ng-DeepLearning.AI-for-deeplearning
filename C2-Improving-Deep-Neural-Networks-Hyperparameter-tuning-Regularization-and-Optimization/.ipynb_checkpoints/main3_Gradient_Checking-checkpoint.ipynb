{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 梯度检验\n",
    "\n",
    "欢迎来到本周的最终作业！在此作业中，您将学习如何实施和使用梯度检验。假设您是致力于在全球范围内提供移动支付的团队的一员，并被要求构建一个深度学习模型来检测欺诈--每当有人付款时，您想要查看付款是否可能是欺诈性的，例如用户的帐户已被黑客接管。但反向传播实施起来非常具有挑战性，有时还会出现错误。由于这是一项关键任务应用程序，因此贵公司的CEO希望确保您的反向传播实施是正确的。你的CEO说，“给我一个证据证明你的反向传播确实在起作用！”为了保证这一点，您将使用“梯度检查”。我们开始做吧！\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入相关库\n",
    "import numpy as np\n",
    "from testCases import *\n",
    "import gc_utils\n",
    "from gc_utils import sigmoid, relu, dictionary_to_vector, vector_to_dictionary, gradients_to_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、怎么样进行梯度检验 ？\n",
    "\n",
    "反向传播计算梯度$\\frac{\\partial J}{\\partial \\theta}$, 其中 $\\theta$ 表示模型中的参数，使用前向传播和损失函数计算 $J$ 。\n",
    "\n",
    "因为向前传播相对容易实现，所以您确信自己得到了正确的结果，从而您几乎100％确保您正确计算了价值函数$J$。 因此，您可以使用您用来计算 $J$ 的代码来检验计算$\\frac{\\partial J}{\\partial \\theta}$的代码是否正确。\n",
    "\n",
    "让我们回头看一下导数（或梯度）的定义：\n",
    "$$ \\frac{\\partial J}{\\partial \\theta} = \\lim_{\\varepsilon \\to 0} \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon} \\tag{1}$$\n",
    "\n",
    "- $\\frac{\\partial J}{\\partial \\theta}$ 是你想确保你的计算正确的值。\n",
    "- 你可以计算 $J(\\theta + \\varepsilon)$ 和  $J(\\theta - \\varepsilon)$ (在 $\\theta$是一个实数的情况下), 因为你确信你对$J$的实现是正确的。\n",
    "\n",
    "让我们用公式 (1) 和一个非常小的值$\\varepsilon$ 来向你的 CEO证明你计算 $\\frac{\\partial J}{\\partial \\theta}$的代码是正确的。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、 1维梯度检验\n",
    "\n",
    "对于1维线性函数模型 $J(\\theta) = \\theta x$，它只包含一个实参 $\\theta$,  $x$是输入。\n",
    "\n",
    "接下来会用代码来实现$J(.)$ 和它的导数 $\\frac{\\partial J}{\\partial \\theta}$的计算。然后用梯度检验来确保对$J$的梯度计算是正确的。 \n",
    "\n",
    "<img src=\"images/1Dgrad_kiank.png\" style=\"width:600px;height:250px;\">\n",
    "<caption><center> <u> **图 1** </u>: **1维线性模型**<br> </center></caption>\n",
    "\n",
    "上图显示了关键计算步骤：首先，从$x$开始计算出函数$J(x)$（前向传播）。然后，计算它的梯度$\\frac{\\partial J}{\\partial \\theta}$ （后向传播）。\n",
    "\n",
    "**内容**: 对这个简单的函数实现前向传播和后向传播。比如，用两个独立的函数同时计算 $J(.)$和它对$\\theta$的导数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(x,theta):\n",
    "    \"\"\"\n",
    "\n",
    "    实现图中呈现的线性前向传播（计算J）（J（theta）= theta * x）\n",
    "\n",
    "    参数：\n",
    "    x  - 一个实值输入\n",
    "    theta  - 参数，也是一个实数\n",
    "\n",
    "    返回：\n",
    "    J  - 函数J的值，用公式J（theta）= theta * x计算\n",
    "    \"\"\"\n",
    "    J = np.dot(theta,x)\n",
    "\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试一下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------测试forward_propagation-----------------\n",
      "J = 8\n"
     ]
    }
   ],
   "source": [
    "#测试forward_propagation\n",
    "print(\"-----------------测试forward_propagation-----------------\")\n",
    "x, theta = 2, 4\n",
    "J = forward_propagation(x, theta)\n",
    "print (\"J = \" + str(J))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前向传播有了，我们来看一下反向传播："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(x,theta):\n",
    "    \"\"\"\n",
    "    计算J相对于θ的导数。\n",
    "\n",
    "    参数：\n",
    "        x  - 一个实值输入\n",
    "        theta  - 参数，也是一个实数\n",
    "\n",
    "    返回：\n",
    "        dtheta  - 相对于θ的成本梯度\n",
    "    \"\"\"\n",
    "    dtheta = x\n",
    "\n",
    "    return dtheta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试一下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------测试backward_propagation-----------------\n",
      "dtheta = 2\n"
     ]
    }
   ],
   "source": [
    "#测试backward_propagation\n",
    "print(\"-----------------测试backward_propagation-----------------\")\n",
    "x, theta = 2, 4\n",
    "dtheta = backward_propagation(x, theta)\n",
    "print (\"dtheta = \" + str(dtheta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**内容**: 为了证明 `backward_propagation()` 函数正确计算了梯度$\\frac{\\partial J}{\\partial \\theta}$, 下面实现梯度检验。\n",
    "\n",
    "**介绍**:\n",
    "- 首先，用公式（1）和一个非常小的值 $\\varepsilon$来计算 \"gradapprox\" 。有如下2个步骤：\n",
    "    1. $\\theta^{+} = \\theta + \\varepsilon$\n",
    "    2. $\\theta^{-} = \\theta - \\varepsilon$\n",
    "    3. $J^{+} = J(\\theta^{+})$\n",
    "    4. $J^{-} = J(\\theta^{-})$\n",
    "    5. $gradapprox = \\frac{J^{+} - J^{-}}{2  \\varepsilon}$\n",
    "- 然后，通过反向传播计算出梯度，并把其结果保存在变量\"grad\"中。\n",
    "- 最后，用下面的公式计算 \"gradapprox\" 和 \"grad\" 的差值:\n",
    "$$ difference = \\frac {\\mid\\mid grad - gradapprox \\mid\\mid_2}{\\mid\\mid grad \\mid\\mid_2 + \\mid\\mid gradapprox \\mid\\mid_2} \\tag{2}$$\n",
    "计算这个公式需要3个步骤：\n",
    "   - 1'. 利用np.linalg.norm(...)计算出分子；\n",
    "   - 2'. 计算分母。需要要调用np.linalg.norm(...) 两次。\n",
    "   - 3'. 让它们相除。\n",
    "- 如果差值非常小 (通常小于$10^{-7}$)，就证明梯度计算正确。否则，梯度计算可能存在错误。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check(x,theta,epsilon=1e-7):\n",
    "    \"\"\"\n",
    "\n",
    "    实现图中的反向传播。\n",
    "\n",
    "    参数：\n",
    "        x  - 一个实值输入\n",
    "        theta  - 参数，也是一个实数\n",
    "        epsilon  - 使用公式（3）计算输入的微小偏移以计算近似梯度\n",
    "\n",
    "    返回：\n",
    "        近似梯度和后向传播梯度之间的差异\n",
    "    \"\"\"\n",
    "\n",
    "    #使用公式（1）的左侧计算gradapprox。\n",
    "    thetaplus = theta + epsilon                               # Step 1\n",
    "    thetaminus = theta - epsilon                              # Step 2\n",
    "    J_plus = forward_propagation(x, thetaplus)                # Step 3\n",
    "    J_minus = forward_propagation(x, thetaminus)              # Step 4\n",
    "    gradapprox = (J_plus - J_minus) / (2 * epsilon)           # Step 5\n",
    "\n",
    "\n",
    "    #检查gradapprox是否足够接近backward_propagation（）的输出\n",
    "    grad = backward_propagation(x, theta)\n",
    "\n",
    "    numerator = np.linalg.norm(grad - gradapprox)                      # Step 1'\n",
    "    denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)    # Step 2'\n",
    "    difference = numerator / denominator                               # Step 3'\n",
    "\n",
    "    if difference < 1e-7:\n",
    "        print(\"梯度检验：梯度正常!\")\n",
    "    else:\n",
    "        print(\"梯度检验：梯度超出阈值!\")\n",
    "\n",
    "    return difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试一下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------测试gradient_check-----------------\n",
      "梯度检验：梯度正常!\n",
      "difference = 2.919335883291695e-10\n"
     ]
    }
   ],
   "source": [
    "#测试gradient_check\n",
    "print(\"-----------------测试gradient_check-----------------\")\n",
    "x, theta = 2, 4\n",
    "difference = gradient_check(x, theta)\n",
    "print(\"difference = \" + str(difference))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三、N维梯度检验\n",
    "\n",
    "下图描述了你的欺诈检测模型的前向传播和后向传播。\n",
    "\n",
    "<img src=\"images/NDgrad_kiank.png\" style=\"width:600px;height:400px;\">\n",
    "<caption><center> <u> **图 2** </u>: **深度神经网络**<br>*LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID*</center></caption>\n",
    "\n",
    "下面是前向传播和后向传播的实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_n(X,Y,parameters):\n",
    "    \"\"\"\n",
    "    实现图中的前向传播（并计算成本）。\n",
    "\n",
    "    参数：\n",
    "        X - 训练集为m个例子\n",
    "        Y -  m个示例的标签\n",
    "        parameters - 包含参数“W1”，“b1”，“W2”，“b2”，“W3”，“b3”的python字典：\n",
    "            W1  - 权重矩阵，维度为（5,4）\n",
    "            b1  - 偏向量，维度为（5,1）\n",
    "            W2  - 权重矩阵，维度为（3,5）\n",
    "            b2  - 偏向量，维度为（3,1）\n",
    "            W3  - 权重矩阵，维度为（1,3）\n",
    "            b3  - 偏向量，维度为（1,1）\n",
    "\n",
    "    返回：\n",
    "        cost - 成本函数（logistic）\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "\n",
    "    # LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID\n",
    "    Z1 = np.dot(W1,X) + b1\n",
    "    A1 = gc_utils.relu(Z1)\n",
    "\n",
    "    Z2 = np.dot(W2,A1) + b2\n",
    "    A2 = gc_utils.relu(Z2)\n",
    "\n",
    "    Z3 = np.dot(W3,A2) + b3\n",
    "    A3 = gc_utils.sigmoid(Z3)\n",
    "\n",
    "    #计算成本\n",
    "    logprobs = np.multiply(-np.log(A3), Y) + np.multiply(-np.log(1 - A3), 1 - Y)\n",
    "    cost = (1 / m) * np.sum(logprobs)\n",
    "\n",
    "    cache = (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3)\n",
    "\n",
    "    return cost, cache\n",
    "\n",
    "def backward_propagation_n(X,Y,cache):\n",
    "    \"\"\"\n",
    "    实现图中所示的反向传播。\n",
    "\n",
    "    参数：\n",
    "        X - 输入数据点（输入节点数量，1）\n",
    "        Y - 标签\n",
    "        cache - 来自forward_propagation_n（）的cache输出\n",
    "\n",
    "    返回：\n",
    "        gradients - 一个字典，其中包含与每个参数、激活和激活前变量相关的成本梯度。\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
    "\n",
    "    dZ3 = A3 - Y\n",
    "    dW3 = (1. / m) * np.dot(dZ3,A2.T)\n",
    "    dW3 = 1. / m * np.dot(dZ3, A2.T)\n",
    "    db3 = 1. / m * np.sum(dZ3, axis=1, keepdims=True)\n",
    "\n",
    "    dA2 = np.dot(W3.T, dZ3)\n",
    "    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n",
    "    #dW2 = 1. / m * np.dot(dZ2, A1.T) * 2  # Should not multiply by 2\n",
    "    dW2 = 1. / m * np.dot(dZ2, A1.T)\n",
    "    db2 = 1. / m * np.sum(dZ2, axis=1, keepdims=True)\n",
    "\n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n",
    "    dW1 = 1. / m * np.dot(dZ1, X.T)\n",
    "    #db1 = 4. / m * np.sum(dZ1, axis=1, keepdims=True) # Should not multiply by 4\n",
    "    db1 = 1. / m * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\n",
    "                 \"dA2\": dA2, \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2,\n",
    "                 \"dA1\": dA1, \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
    "\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你获得了欺诈测试集的结果，但是你对你的模型不是100%确定是正确的。没有人完美的。下面实现梯度检验来检查你的梯度是否正确。\n",
    "\n",
    "**怎么实现梯度检验?**.\n",
    "\n",
    "因为在内容一、二中，是通过比较“gradapprox”与反向传播计算的梯度。 该公式仍然是： \n",
    "$$ \\frac{\\partial J}{\\partial \\theta} = \\lim_{\\varepsilon \\to 0} \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon} \\tag{1}$$\n",
    "然而，$\\theta$ 不再是标量。 这是一个名为“parameters”的字典。 我们为你实现了一个函数“dictionary_to_vector（）”。 它将“parameters”字典转换为一个名为“values”的向量，方法是通过将该字典的所有参数（W1，b1，W2，b2，W3，b3）整形为向量并将它们连接起来而获得。\n",
    "\n",
    "“vector_to_dictionary”是反函数，它返回“parameters”字典。 \n",
    "\n",
    "<img src=\"images/dictionary_to_vector.png\" style=\"width:600px;height:400px;\">\n",
    "<caption><center> <u> **图 2** </u>: **dictionary_to_vector() and vector_to_dictionary()**<br> 在 gradient_check_n()中会需要这些函数</center></caption>\n",
    "\n",
    "我们也用函数gradients_to_vector()把字典 \"gradients\" 转化为向量 \"grad\" 。对此不用担心。\n",
    "\n",
    "**内容**: Implement gradient_check_n().\n",
    "\n",
    "**介绍**: 下面是实现梯度检验的伪代码。\n",
    "\n",
    "For each i in num_parameters:\n",
    "- 计算 `J_plus[i]`:\n",
    "    1. Set $\\theta^{+}$ to `np.copy(parameters_values)`\n",
    "    2. Set $\\theta^{+}_i$ to $\\theta^{+}_i + \\varepsilon$\n",
    "    3. Calculate $J^{+}_i$ using to `forward_propagation_n(x, y, vector_to_dictionary(`$\\theta^{+}$ `))`.     \n",
    "- 计算`J_minus[i]`: 用 $\\theta^{-}$做同样事情 \n",
    "- 计算 $gradapprox[i] = \\frac{J^{+}_i - J^{-}_i}{2 \\varepsilon}$\n",
    "\n",
    "因此，得到一个向量gradapprox, 其中gradapprox[i] 是相对于 `parameter_values[i]`的梯度的近似。然后，比较向量gradapprox和反向传播的梯度向量。就像1维梯度检验 (Steps 1', 2', 3')中一样，计算：\n",
    "$$ difference = \\frac {\\| grad - gradapprox \\|_2}{\\| grad \\|_2 + \\| gradapprox \\|_2 } \\tag{3}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check_n(parameters,gradients,X,Y,epsilon=1e-7):\n",
    "    \"\"\"\n",
    "    检查backward_propagation_n是否正确计算forward_propagation_n输出的成本梯度\n",
    "\n",
    "    参数：\n",
    "        parameters - 包含参数“W1”，“b1”，“W2”，“b2”，“W3”，“b3”的python字典：\n",
    "        grad_output_propagation_n的输出包含与参数相关的成本梯度。\n",
    "        x  - 输入数据点，维度为（输入节点数量，1）\n",
    "        y  - 标签\n",
    "        epsilon  - 计算输入的微小偏移以计算近似梯度\n",
    "\n",
    "    返回：\n",
    "        difference - 近似梯度和后向传播梯度之间的差异\n",
    "    \"\"\"\n",
    "    #初始化参数\n",
    "    parameters_values , keys = gc_utils.dictionary_to_vector(parameters) #keys用不到\n",
    "    grad = gc_utils.gradients_to_vector(gradients)\n",
    "    num_parameters = parameters_values.shape[0]\n",
    "    J_plus = np.zeros((num_parameters,1))\n",
    "    J_minus = np.zeros((num_parameters,1))\n",
    "    gradapprox = np.zeros((num_parameters,1))\n",
    "\n",
    "    #计算gradapprox\n",
    "    for i in range(num_parameters):\n",
    "        #计算J_plus [i]。输入：“parameters_values，epsilon”。输出=“J_plus [i]”\n",
    "        thetaplus = np.copy(parameters_values)                                                  # Step 1\n",
    "        thetaplus[i][0] = thetaplus[i][0] + epsilon                                             # Step 2\n",
    "        J_plus[i], cache = forward_propagation_n(X,Y,gc_utils.vector_to_dictionary(thetaplus))  # Step 3 ，cache用不到\n",
    "\n",
    "        #计算J_minus [i]。输入：“parameters_values，epsilon”。输出=“J_minus [i]”。\n",
    "        thetaminus = np.copy(parameters_values)                                                 # Step 1\n",
    "        thetaminus[i][0] = thetaminus[i][0] - epsilon                                           # Step 2        \n",
    "        J_minus[i], cache = forward_propagation_n(X,Y,gc_utils.vector_to_dictionary(thetaminus))# Step 3 ，cache用不到\n",
    "\n",
    "        #计算gradapprox[i]\n",
    "        gradapprox[i] = (J_plus[i] - J_minus[i]) / (2 * epsilon)\n",
    "\n",
    "    #通过计算差异比较gradapprox和后向传播梯度。\n",
    "    numerator = np.linalg.norm(grad - gradapprox)                                     # Step 1'\n",
    "    denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)                   # Step 2'\n",
    "    difference = numerator / denominator                                              # Step 3'\n",
    "\n",
    "    \n",
    "    if difference > 1e-7:\n",
    "        print (\"\\033[93m\" + \"梯度计算错误！There is a mistake in the backward propagation! difference = \" + str(difference) + \"\\033[0m\")\n",
    "    else:\n",
    "        print (\"\\033[92m\" + \"梯度计算正确！Your backward propagation works perfectly fine! difference = \" + str(difference) + \"\\033[0m\")\n",
    "\n",
    "    return difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m梯度计算错误！There is a mistake in the backward propagation! difference = 1.1890417878779317e-07\u001b[0m\n",
      "difference = 1.1890417878779317e-07\n"
     ]
    }
   ],
   "source": [
    "X, Y, parameters = gradient_check_n_test_case()\n",
    "\n",
    "cost, cache = forward_propagation_n(X, Y, parameters)\n",
    "gradients = backward_propagation_n(X, Y, cache)\n",
    "difference = gradient_check_n(parameters, gradients, X, Y)\n",
    "print(\"difference = \"+ str(difference))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们给你的`back_propagation_n`代码似乎有错误！很好，你已经实现了梯度检验。返回`backward_propagation`并尝试查找/更正错误（提示：检查dW2和db1）。当您认为已修复时，请重新运行梯度检验。请记住，如果修改代码，则需要重新执行定义`backward_propagation_n（）`的单元格。你能通过梯度检验来证明你的导数计算是否正确？即使这部分作业没有评分，我们强烈建议您尝试找到错误并重新运行梯度检验，直到您确信后向传播已正确实现。 \n",
    "    \n",
    "    **注意** \n",
    "    \n",
    "   - 梯度检验很慢！使用$\\frac{\\partial J}{\\partial \\theta} \\approx  \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon}$ 计算近似梯度成本高昂。因此，我们不会在训练期间的每次迭代中运行梯度检验。只需检查几次梯度是否正确。 \n",
    "   - 至少在我们提出的情况下，梯度检验不适用于dropout。您通常会在没有dropout的情况下运行梯度检验算法，以确保您的反向传播是正确的，然后添加dropout。\n",
    "   \n",
    "   恭喜，您可以确保您的欺诈检测深度学习模型完全正确！你甚至可以用它来说服你的CEO。 :) \n",
    "   \n",
    "   <font color ='blue'> \n",
    "    \n",
    " **你应该记住的笔记**： \n",
    "    - 梯度检验验证来自反向传播的梯度与梯度的数值近似（使用前向传播计算）之间的接近程度。 \n",
    "    - 梯度检验很慢，因此我们不会在每次训练迭代中运行它。通常只需运行它以确保您的代码是正确的，然后将其关闭并使用backprop进行实际的学习过程。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
